{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Funciones Auxiliares (Basadas en Clase_NNA3) ---\n",
        "def sig(s):\n",
        "    # Función de activación sigmoide\n",
        "    return 1 / (1 + np.exp(-s))\n",
        "\n",
        "def dSig(s):\n",
        "    # Derivada de la función sigmoide\n",
        "    # s debe ser un vector/matriz de activaciones ya calculadas o potenciales\n",
        "    # Ajustado para que funcione con la salida de sig(Z) o Z directamente si se maneja con cuidado\n",
        "    # En el original: df = sig(s)*(sig(s)), pero dSig se suele aplicar a sig(z).\n",
        "    # Usaremos la forma: f'(z) = f(z)(1-f(z)) si s es A, o sig(s)(1-sig(s)) si s es Z.\n",
        "    # Siguiendo tu código original, se pasaba Z.\n",
        "    val = sig(s)\n",
        "    return val * (1 - val) # Derivada estándar: sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def error(A_final, Yd):\n",
        "    return Yd - A_final\n",
        "\n",
        "# --- Clase Red Neuronal con 2 Capas Ocultas ---\n",
        "class RedNeuronalDosOcultas:\n",
        "    def __init__(self, nn_input, nn_hidden1, nn_hidden2, nn_output, eta=0.5):\n",
        "        self.eta = eta\n",
        "\n",
        "        # Inicialización aleatoria de pesos y sesgos para 3 conjuntos de conexiones\n",
        "        # Capa 1: Entrada -> Oculta 1\n",
        "        self.W1 = np.random.uniform(-1, 1, (nn_hidden1, nn_input))\n",
        "        self.b1 = np.random.uniform(-1, 1, (nn_hidden1, 1))\n",
        "\n",
        "        # Capa 2: Oculta 1 -> Oculta 2\n",
        "        self.W2 = np.random.uniform(-1, 1, (nn_hidden2, nn_hidden1))\n",
        "        self.b2 = np.random.uniform(-1, 1, (nn_hidden2, 1))\n",
        "\n",
        "        # Capa 3: Oculta 2 -> Salida\n",
        "        self.W3 = np.random.uniform(-1, 1, (nn_output, nn_hidden2))\n",
        "        self.b3 = np.random.uniform(-1, 1, (nn_output, 1))\n",
        "\n",
        "    def propaga(self, X):\n",
        "        # Capa 1\n",
        "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
        "        self.A1 = sig(self.Z1)\n",
        "\n",
        "        # Capa 2\n",
        "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
        "        self.A2 = sig(self.Z2)\n",
        "\n",
        "        # Capa 3 (Salida)\n",
        "        self.Z3 = np.dot(self.W3, self.A2) + self.b3\n",
        "        self.A3 = sig(self.Z3)\n",
        "\n",
        "        return self.A3\n",
        "\n",
        "    def backpropagation(self, X, Yd):\n",
        "        # El error es Yd - A3\n",
        "        err = error(self.A3, Yd)\n",
        "\n",
        "        # --- Retropropagación Capa 3 (Salida) ---\n",
        "        # delta3 = dSig(Z3) * error. (Nota: en tu código usabas dSig con matriz diagonal, simplificamos con multiplicación elemento a elemento que es equivalente para vectores)\n",
        "        delta3 = err * dSig(self.Z3)\n",
        "        dEdW3 = -np.dot(delta3, self.A2.T)\n",
        "        dEdb3 = -delta3\n",
        "\n",
        "        # --- Retropropagación Capa 2 (Oculta 2) ---\n",
        "        # El error se propaga hacia atrás a través de W3\n",
        "        delta2 = np.dot(self.W3.T, delta3) * dSig(self.Z2)\n",
        "        dEdW2 = -np.dot(delta2, self.A1.T)\n",
        "        dEdb2 = -delta2\n",
        "\n",
        "        # --- Retropropagación Capa 1 (Oculta 1) ---\n",
        "        # El error se propaga hacia atrás a través de W2\n",
        "        delta1 = np.dot(self.W2.T, delta2) * dSig(self.Z1)\n",
        "        dEdW1 = -np.dot(delta1, X.T)\n",
        "        dEdb1 = -delta1\n",
        "\n",
        "        return dEdW1, dEdb1, dEdW2, dEdb2, dEdW3, dEdb3\n",
        "\n",
        "    def train(self, X_train, Y_train, epochs=10000):\n",
        "        for i in range(epochs):\n",
        "            # Seleccionamos un ejemplo aleatorio o iteramos todos (aquí iteramos todos)\n",
        "            # Para generalizar, usaremos Stochastic Gradient Descent (uno a uno)\n",
        "            idx = np.random.randint(0, X_train.shape[1])\n",
        "            X_sample = X_train[:, [idx]] # Mantiene dimensión columna (n, 1)\n",
        "            Y_sample = Y_train[:, [idx]]\n",
        "\n",
        "            # 1. Propagación\n",
        "            self.propaga(X_sample)\n",
        "\n",
        "            # 2. Retropropagación\n",
        "            dEdW1, dEdb1, dEdW2, dEdb2, dEdW3, dEdb3 = self.backpropagation(X_sample, Y_sample)\n",
        "\n",
        "            # 3. Actualización de pesos\n",
        "            self.W3 -= self.eta * dEdW3\n",
        "            self.b3 -= self.eta * dEdb3\n",
        "            self.W2 -= self.eta * dEdW2\n",
        "            self.b2 -= self.eta * dEdb2\n",
        "            self.W1 -= self.eta * dEdW1\n",
        "            self.b1 -= self.eta * dEdb1\n",
        "\n",
        "    def test(self, X_test):\n",
        "        results = []\n",
        "        print(f\"\\n--- Probando Red ---\")\n",
        "        for i in range(X_test.shape[1]):\n",
        "            x = X_test[:, [i]]\n",
        "            pred = self.propaga(x)\n",
        "            print(f\"Entrada: {x.T} -> Salida Red: {pred.T[0]} (Redondeado: {np.round(pred.T[0])})\")\n",
        "            results.append(pred)\n",
        "        return results\n",
        "\n",
        "# ==========================================\n",
        "# DATOS DE ENTRENAMIENTO (Tablas de Verdad)\n",
        "# ==========================================\n",
        "# Entradas (iguales para ambas): 00, 01, 10, 11\n",
        "X_data = np.array([\n",
        "    [0, 0, 1, 1],\n",
        "    [0, 1, 0, 1]\n",
        "])\n",
        "\n",
        "# Salidas Deseadas NAND (1 si no son ambos 1)\n",
        "Y_NAND = np.array([[1, 1, 1, 0]])\n",
        "\n",
        "# Salidas Deseadas XOR (1 si son diferentes)\n",
        "Y_XOR = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "# ==========================================\n",
        "# RED NEURONAL 1: APRENDIENDO NAND\n",
        "# ==========================================\n",
        "print(\"\\n>>> ENTRENANDO RED NAND <<<\")\n",
        "# Arquitectura: 2 entradas, 4 oculta1, 4 oculta2, 1 salida\n",
        "nn_nand = RedNeuronalDosOcultas(nn_input=2, nn_hidden1=4, nn_hidden2=4, nn_output=1, eta=0.5)\n",
        "nn_nand.train(X_data, Y_NAND, epochs=20000)\n",
        "nn_nand.test(X_data)\n",
        "\n",
        "# ==========================================\n",
        "# RED NEURONAL 2: APRENDIENDO XOR\n",
        "# ==========================================\n",
        "print(\"\\n>>> ENTRENANDO RED XOR <<<\")\n",
        "# XOR es más complejo, a veces requiere más épocas o neuronas\n",
        "nn_xor = RedNeuronalDosOcultas(nn_input=2, nn_hidden1=8, nn_hidden2=8, nn_output=1, eta=0.5)\n",
        "nn_xor.train(X_data, Y_XOR, epochs=30000)\n",
        "nn_xor.test(X_data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Azq5JTyJsF_9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}