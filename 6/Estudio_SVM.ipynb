{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ==============================================================================\n",
        "# EJERCICIO 2: ESTUDIO DETALLADO Y APLICACIÓN DE SVM (Support Vector Machine)\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"\n",
        "--- DOCUMENTACIÓN MEJORADA DEL ALGORITMO SVM ---\n",
        "\n",
        "1. ¿QUÉ ES SVM?\n",
        "   Es un algoritmo de aprendizaje supervisado potente para clasificación y regresión.\n",
        "   Su objetivo fundamental es encontrar el HIPERPLANO óptimo que separa las clases\n",
        "   en el espacio de características.\n",
        "\n",
        "2. CONCEPTOS CLAVE:\n",
        "\n",
        "   A. Hiperplano:\n",
        "      Es la frontera de decisión. En 2D es una línea, en 3D un plano.\n",
        "      Todo lo que cae a un lado se clasifica como Clase A, y al otro como Clase B.\n",
        "\n",
        "   B. Vectores de Soporte (Support Vectors):\n",
        "      Son los puntos de datos (instancias) más difíciles de clasificar porque\n",
        "      están más cerca de la línea divisoria. Son los puntos \"críticos\".\n",
        "      El algoritmo SVM se llama así porque se apoya exclusivamente en estos puntos\n",
        "      para definir el hiperplano; los puntos lejanos no importan.\n",
        "\n",
        "   C. Margen (Margin):\n",
        "      Es la distancia entre el hiperplano y los vectores de soporte más cercanos.\n",
        "      SVM busca MAXIMIZAR este margen. Un margen más amplio significa que el modelo\n",
        "      es más robusto y generaliza mejor.\n",
        "\n",
        "   D. Kernel Trick (Truco del Kernel):\n",
        "      ¿Qué pasa si los datos no se pueden separar con una línea recta (no son linealmente separables)?\n",
        "      SVM proyecta los datos a una dimensión superior (ej: de 2D a 3D) donde sí\n",
        "      se puedan separar linealmente.\n",
        "      - Linear: Para datos simples.\n",
        "      - RBF (Radial Basis Function): El más popular para fronteras curvas complejas.\n",
        "      - Polynomial: Crea fronteras curvas de grado específico.\n",
        "\n",
        "   E. Hiperparámetros Críticos (C y Gamma):\n",
        "      - C (Regularización):\n",
        "        - C alto: Intenta clasificar todo correctamente (riesgo de Overfitting/Sobreajuste). Margen estrecho.\n",
        "        - C bajo: Permite algunos errores para buscar un margen más ancho (Underfitting/Suave).\n",
        "      - Gamma (Solo para kernels RBF/Poly):\n",
        "        - Gamma alto: Considera solo puntos muy cercanos al hiperplano (fronteras muy curvas y ajustadas).\n",
        "        - Gamma bajo: Considera puntos lejanos (fronteras más suaves y rectas).\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# APLICACIÓN PRÁCTICA: CLASIFICACIÓN DE VINOS\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Carga de Datos\n",
        "# Usamos el dataset de Vinos (Wine Dataset), un problema clásico multiclase.\n",
        "# 13 características químicas (alcohol, málico, cenizas...) para clasificar 3 tipos de vinos.\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "target_names = wine.target_names\n",
        "\n",
        "print(f\"Dataset cargado: {X.shape[0]} muestras, {X.shape[1]} características.\")\n",
        "print(f\"Clases de vino: {target_names}\")\n",
        "\n",
        "# 2. Preprocesamiento\n",
        "# SVM es MUY sensible a la escala de los datos. Si una característica tiene valores de 1000\n",
        "# y otra de 0.1, la de 1000 dominará el cálculo de distancias. ES OBLIGATORIO ESCALAR.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Configuración y Entrenamiento del Modelo SVM\n",
        "# Probamos con un Kernel RBF que es versátil para datos reales complejos.\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "\n",
        "print(\"\\nEntrenando SVM con kernel RBF...\")\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. Evaluación\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n--- REPORTE DE CLASIFICACIÓN ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión Global (Accuracy): {acc*100:.2f}%\")\n",
        "\n",
        "# ==============================================================================\n",
        "# VISUALIZACIÓN (Reduciendo a 2 dimensiones para poder ver el Hiperplano)\n",
        "# ==============================================================================\n",
        "# Para visualizar los márgenes, entrenaremos un nuevo SVM usando solo las 2 primeras features.\n",
        "X_vis = X_train_scaled[:, :2]  # Solo Alcohol y Ácido Málico\n",
        "y_vis = y_train\n",
        "\n",
        "svm_vis = SVC(kernel='linear', C=1.0) # Usamos linear aquí para ver líneas claras\n",
        "svm_vis.fit(X_vis, y_vis)\n",
        "\n",
        "# Crear malla para plotear\n",
        "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "Z = svm_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "plt.title('SVM: Fronteras de Decisión (Solo 2 características para visualización)')\n",
        "plt.xlabel('Alcohol (Estandarizado)')\n",
        "plt.ylabel('Ácido Málico (Estandarizado)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCONCLUSIÓN SVM:\")\n",
        "print(\"El algoritmo logra separar eficazmente las clases maximizando el margen.\")\n",
        "print(\"La estandarización (StandardScaler) fue el paso crítico para obtener buenos resultados.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kmfsLKqHt_8Y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}