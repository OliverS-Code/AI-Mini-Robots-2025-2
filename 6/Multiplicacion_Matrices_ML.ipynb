{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==============================================================================\n",
        "# EJERCICIO 1: APRENDIZAJE DE REGLAS MATEMÁTICAS (REF: FIGURA 6.1)\n",
        "# ==============================================================================\n",
        "# Objetivo: Demostrar cómo ML aprende \"Reglas\" a partir de \"Datos\" y \"Respuestas\".\n",
        "# Problema: Aprender a multiplicar matrices 2x2.\n",
        "# Entrada: 8 números (dos matrices 2x2 aplanadas).\n",
        "# Salida: 4 números (matriz resultado 2x2 aplanada).\n",
        "\n",
        "print(\"--- GENERANDO DATASET ---\")\n",
        "# 1. Generación de Datos\n",
        "# ------------------------------------------------------------------------------\n",
        "NUM_SAMPLES = 50000\n",
        "RANGO = 20 # Enteros entre -20 y 20\n",
        "\n",
        "# Generamos matrices A y B aleatorias\n",
        "# Forma: (50000, 2, 2)\n",
        "A = np.random.randint(-RANGO, RANGO + 1, size=(NUM_SAMPLES, 2, 2))\n",
        "B = np.random.randint(-RANGO, RANGO + 1, size=(NUM_SAMPLES, 2, 2))\n",
        "\n",
        "# Calculamos el resultado real (Ground Truth) usando la regla analítica (np.matmul)\n",
        "C = np.matmul(A, B)\n",
        "\n",
        "# 2. Preprocesamiento\n",
        "# ------------------------------------------------------------------------------\n",
        "# Las redes neuronales funcionan mejor con datos aplanados y normalizados.\n",
        "# Aplanamos: Cada entrada será un vector de 8 elementos (4 de A + 4 de B)\n",
        "# Salida: Vector de 4 elementos (elementos de C)\n",
        "\n",
        "X = np.concatenate([A.reshape(NUM_SAMPLES, -1), B.reshape(NUM_SAMPLES, -1)], axis=1)\n",
        "y = C.reshape(NUM_SAMPLES, -1)\n",
        "\n",
        "print(f\"Dimensiones de X (Entrada): {X.shape}\") # Debería ser (50000, 8)\n",
        "print(f\"Dimensiones de y (Salida):  {y.shape}\") # Debería ser (50000, 4)\n",
        "\n",
        "# Normalización: Es crucial para que la red converja, especialmente con números grandes.\n",
        "# Como el rango de entrada es aprox [-20, 20], dividimos por 20.\n",
        "# El rango de salida puede ser mayor (ej: 20*20 + 20*20 = 800), normalizamos aprox.\n",
        "scale_in = 20.0\n",
        "scale_out = 800.0 # Estimación burda del máximo valor posible (20*20 + 20*20)\n",
        "\n",
        "X_norm = X / scale_in\n",
        "y_norm = y / scale_out\n",
        "\n",
        "# Split entrenamiento/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Diseño del Modelo\n",
        "# ------------------------------------------------------------------------------\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(8,)), # 8 Entradas\n",
        "    tf.keras.layers.Dense(64, activation='relu'), # Capa oculta con capacidad suficiente\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='linear')  # 4 Salidas (Regresión lineal, sin activación acotada)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 4. Entrenamiento\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n--- ENTRENANDO MODELO (Buscando la 'Regla') ---\")\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.1)\n",
        "print(\"Entrenamiento completado.\")\n",
        "\n",
        "# 5. Evaluación y Comparación\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n--- PRUEBA CON 10 EJEMPLOS NUEVOS ---\")\n",
        "\n",
        "# Generamos 10 ejemplos nuevos aleatorios para probar\n",
        "A_new = np.random.randint(-RANGO, RANGO + 1, size=(10, 2, 2))\n",
        "B_new = np.random.randint(-RANGO, RANGO + 1, size=(10, 2, 2))\n",
        "C_real = np.matmul(A_new, B_new)\n",
        "\n",
        "# Preparamos para la red\n",
        "X_new = np.concatenate([A_new.reshape(10, -1), B_new.reshape(10, -1)], axis=1)\n",
        "X_new_norm = X_new / scale_in\n",
        "\n",
        "# Predicción\n",
        "pred_norm = model.predict(X_new_norm)\n",
        "# Des-normalizamos la predicción\n",
        "pred_real = pred_norm * scale_out\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"{'Real (Analítico)':<35} | {'Predicción (ML)':<35} | {'Dif Abs (Error)'}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "errores = []\n",
        "for i in range(10):\n",
        "    real_flat = C_real[i].flatten()\n",
        "    pred_flat = pred_real[i]\n",
        "\n",
        "    # Redondeamos la predicción a enteros para comparar visualmente mejor\n",
        "    pred_rounded = np.round(pred_flat).astype(int)\n",
        "\n",
        "    diff = np.abs(real_flat - pred_flat)\n",
        "    errores.append(np.mean(diff))\n",
        "\n",
        "    print(f\"{str(real_flat):<35} | {str(pred_rounded):<35} | {np.mean(diff):.2f}\")\n",
        "\n",
        "print(\"-\" * 90)\n",
        "print(f\"Error medio absoluto promedio: {np.mean(errores):.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CONCLUSIONES\n",
        "# ==============================================================================\n",
        "\"\"\"\n",
        "CONCLUSIONES DEL EXPERIMENTO:\n",
        "\n",
        "1. Aproximación vs Exactitud:\n",
        "   A diferencia de la programación tradicional (método analítico) que da resultados\n",
        "   exactos siempre, el modelo de ML realiza una 'aproximación de la función'.\n",
        "   Aunque los resultados son muy cercanos (el error suele ser bajo), rara vez\n",
        "   son exactamente iguales en punto flotante, aunque al redondear coinciden.\n",
        "\n",
        "2. Generalización (Figura 6.1):\n",
        "   El modelo ha logrado inferir la relación compleja (filas por columnas)\n",
        "   sin que nosotros programáramos explícitamente los bucles de multiplicación.\n",
        "   Ha extraído las 'reglas' a partir de los 50,000 ejemplos.\n",
        "\n",
        "3. Dependencia de los Datos:\n",
        "   Si probáramos con números fuera del rango [-20, 20] (ej: 1000), el modelo\n",
        "   probablemente fallaría porque la red neuronal no extrapola bien fuera del\n",
        "   rango de normalización aprendido. La fórmula analítica, en cambio, es universal.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1ORodpfitrvh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}