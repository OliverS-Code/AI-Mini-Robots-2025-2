{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ==========================================\n",
        "# 1. OBTENCIÓN Y ESTUDIO DEL DATASET\n",
        "# ==========================================\n",
        "# Cargamos el dataset Breast Cancer Wisconsin\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Extraemos las características (X) y el rótulo (y)\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convertimos a DataFrame para facilitar el análisis visual\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"--- INFORMACIÓN DEL DATASET ---\")\n",
        "print(f\"Número de muestras: {df.shape[0]}\")\n",
        "print(f\"Número de características (features): {df.shape[1] - 1}\") # Restamos la columna target\n",
        "print(f\"Nombres de clases: {data.target_names}\") # ['malignant' 'benign']\n",
        "print(\"\\n--- PRIMERAS 5 FILAS (VISTA PREVIA DE CARACTERÍSTICAS) ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- ESTADÍSTICAS DESCRIPTIVAS ---\")\n",
        "print(df.describe().transpose().head(10)) # Mostramos solo las primeras 10 para no saturar\n",
        "\n",
        "# ==========================================\n",
        "# 2. PREPROCESAMIENTO\n",
        "# ==========================================\n",
        "# Dividir en entrenamiento (80%) y prueba (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ESTANDARIZACIÓN (CRÍTICO):\n",
        "# Las redes neuronales requieren que las entradas tengan escalas similares.\n",
        "# StandardScaler ajusta los datos para que tengan media 0 y varianza 1.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nDimensiones de X_train (escalado): {X_train_scaled.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DISEÑO DE LA RED NEURONAL\n",
        "# ==========================================\n",
        "model = tf.keras.Sequential([\n",
        "    # Capa de Entrada implícita (30 neuronas, una por característica)\n",
        "\n",
        "    # Capa Oculta 1: 16 neuronas, activación ReLU\n",
        "    # Decisión de diseño: Suficiente capacidad para aprender patrones no lineales en 30 features.\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(30,)),\n",
        "\n",
        "    # Capa Oculta 2: 8 neuronas, activación ReLU\n",
        "    # Reduce la dimensionalidad progresivamente.\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "\n",
        "    # Capa de Salida: 1 neurona, activación Sigmoide\n",
        "    # Razón: Es clasificación binaria (0 o 1). La sigmoide da una probabilidad entre 0 y 1.\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy', # Pérdida estándar para clasificación binaria\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ==========================================\n",
        "# 4. ENTRENAMIENTO\n",
        "# ==========================================\n",
        "print(\"\\n--- INICIANDO ENTRENAMIENTO ---\")\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
        "print(\"Entrenamiento finalizado.\")\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nPrecisión en el set de prueba: {accuracy*100:.2f}%\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. EJEMPLOS BASADOS EN PESOS APRENDIDOS\n",
        "# ==========================================\n",
        "\n",
        "# A. Predicciones con el modelo entrenado\n",
        "# Tomamos 5 muestras del set de prueba para ver qué decide la red\n",
        "print(\"\\n--- EJEMPLOS DE PREDICCIÓN ---\")\n",
        "sample_indices = [0, 10, 20, 30, 40]\n",
        "samples = X_test_scaled[sample_indices]\n",
        "true_labels = y_test[sample_indices]\n",
        "\n",
        "# La red usa sus pesos aprendidos para calcular esto:\n",
        "predictions = model.predict(samples)\n",
        "\n",
        "print(f\"{'Probabilidad (Benigno)':<25} | {'Predicción':<12} | {'Realidad':<12}\")\n",
        "print(\"-\" * 55)\n",
        "for i, pred in enumerate(predictions):\n",
        "    prob = pred[0]\n",
        "    predicted_label = 1 if prob > 0.5 else 0\n",
        "    clase_pred = data.target_names[predicted_label]\n",
        "    clase_real = data.target_names[true_labels[i]]\n",
        "    print(f\"{prob:<25.4f} | {clase_pred:<12} | {clase_real:<12}\")\n",
        "\n",
        "# B. Inspección de los Pesos (Weights)\n",
        "# Podemos ver qué características está ponderando más la primera capa.\n",
        "print(\"\\n--- ANÁLISIS DE PESOS (PRIMERA CAPA) ---\")\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "print(f\"Forma de la matriz de pesos (Features x Neuronas): {weights.shape}\")\n",
        "\n",
        "# Calculamos la magnitud promedio de los pesos para cada característica de entrada\n",
        "# Esto nos da una idea aproximada de la \"importancia\" que la red le dio a cada feature.\n",
        "feature_importance = np.mean(np.abs(weights), axis=1)\n",
        "\n",
        "# Creamos un DataFrame para visualizarlo mejor\n",
        "importance_df = pd.DataFrame({\n",
        "    'Característica': data.feature_names,\n",
        "    'Peso_Promedio': feature_importance\n",
        "}).sort_values(by='Peso_Promedio', ascending=False)\n",
        "\n",
        "print(\"\\nTop 5 Características más influyentes según los pesos aprendidos:\")\n",
        "print(importance_df.head(5))\n",
        "\n",
        "# Visualización rápida de la pérdida durante el entrenamiento\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Pérdida (Train)')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida (Val)')\n",
        "plt.title('Curva de Aprendizaje (Pérdida)')\n",
        "plt.xlabel('Épocas')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UqDtG2Qhs7ZT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}